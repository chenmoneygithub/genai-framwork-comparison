{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFVvH5-8chUM",
        "outputId": "85436ac0-fb0e-4ef2-ff33-813de7202732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/192.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m184.3/192.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m730.3/730.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.8/288.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.1/374.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU pydantic-ai[logfire] openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!logfire auth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV5N4sm3_O-p",
        "outputId": "af2e84f5-6763-43ad-cd9d-da0ea4a05259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Welcome to Logfire! ðŸ”¥\n",
            "Before you can send data to Logfire, we need to authenticate you.\n",
            "\n",
            "Logfire is available in multiple data regions. Please select one:\n",
            "1. US (GCP region: us-east4)\n",
            "2. EU (GCP region: europe-west4)\n",
            "Selected region [1/2]: 1\n",
            "Press Enter to open logfire-us.pydantic.dev in your browser...\n",
            "Please open https://logfire-us.pydantic.dev/auth/device/7VjbSkC5E4U80l2vHha3RjVHQPEnY1HKhcPMifFp9uc in your browser to authenticate if it hasn't already.\n",
            "Waiting for you to authenticate with Logfire...\n",
            "Successfully authenticated!\n",
            "\n",
            "Your Logfire credentials are stored in /root/.logfire/default.toml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!logfire projects use starter-project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNeaZqSh_p-6",
        "outputId": "90e6ae8b-c24e-462c-e423-020d01abc2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project configured successfully. You will be able to view it at: https://logfire-us.pydantic.dev/qianchen94/starter-project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "openai_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key"
      ],
      "metadata": {
        "id": "u5rw9b1x2Bk7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logfire\n",
        "\n",
        "logfire.configure()\n",
        "logfire.instrument_pydantic_ai()"
      ],
      "metadata": {
        "id": "zA1uNJgP7oSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Start"
      ],
      "metadata": {
        "id": "-LLqLEYo8Quu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "from datetime import date\n",
        "\n",
        "from pydantic_ai import Agent\n",
        "from pydantic_ai.tools import RunContext\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class WeatherService:\n",
        "    async def get_forecast(self, location: str, forecast_date: date) -> str:\n",
        "        # In real code: call weather API, DB queries, etc.\n",
        "        return f'The forecast in {location} on {forecast_date} is 24Â°C and sunny.'\n",
        "\n",
        "    async def get_historic_weather(self, location: str, forecast_date: date) -> str:\n",
        "        # In real code: call a historical weather API or DB\n",
        "        return (\n",
        "            f'The weather in {location} on {forecast_date} was 18Â°C and partly cloudy.'\n",
        "        )\n",
        "\n",
        "\n",
        "weather_agent = Agent[WeatherService, str](\n",
        "    'openai:gpt-4o-mini',\n",
        "    deps_type=WeatherService,\n",
        "    output_type=str,  # We'll produce a final answer as plain text\n",
        "    system_prompt='Providing a weather forecast at the locations the user provides.',\n",
        ")\n",
        "\n",
        "\n",
        "@weather_agent.tool\n",
        "async def weather_forecast(\n",
        "    ctx: RunContext[WeatherService],\n",
        "    location: str,\n",
        "    forecast_date: date,\n",
        ") -> str:\n",
        "    if forecast_date >= date.today():\n",
        "        return await ctx.deps.get_forecast(location, forecast_date)\n",
        "    else:\n",
        "        return await ctx.deps.get_historic_weather(location, forecast_date)\n",
        "\n",
        "\n",
        "output = await weather_agent.run(\n",
        "    'What will the weather be like in Paris on Tuesday?',\n",
        "    deps=WeatherService(),\n",
        ")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFplPT6l8R3e",
        "outputId": "5b2cdc42-e09a-474d-d130-027ccaac9e91"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentRunResult(output='The weather in Paris on Tuesday, October 31, 2023, will be 18Â°C and partly cloudy.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Start with Streaming\n",
        "\n",
        "Below is a quick start example with streaming turned on. It's not quite \"quick\" because streaming requires some additional handling."
      ],
      "metadata": {
        "id": "_S4ELcW43CSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from dataclasses import dataclass\n",
        "from datetime import date\n",
        "\n",
        "from pydantic_ai import Agent\n",
        "from pydantic_ai.messages import (\n",
        "    FinalResultEvent,\n",
        "    FunctionToolCallEvent,\n",
        "    FunctionToolResultEvent,\n",
        "    PartDeltaEvent,\n",
        "    PartStartEvent,\n",
        "    TextPartDelta,\n",
        "    ToolCallPartDelta,\n",
        ")\n",
        "from pydantic_ai.tools import RunContext\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class WeatherService:\n",
        "    async def get_forecast(self, location: str, forecast_date: date) -> str:\n",
        "        # In real code: call weather API, DB queries, etc.\n",
        "        return f'The forecast in {location} on {forecast_date} is 24Â°C and sunny.'\n",
        "\n",
        "    async def get_historic_weather(self, location: str, forecast_date: date) -> str:\n",
        "        # In real code: call a historical weather API or DB\n",
        "        return (\n",
        "            f'The weather in {location} on {forecast_date} was 18Â°C and partly cloudy.'\n",
        "        )\n",
        "\n",
        "\n",
        "weather_agent = Agent[WeatherService, str](\n",
        "    'openai:gpt-4o-mini',\n",
        "    deps_type=WeatherService,\n",
        "    output_type=str,  # We'll produce a final answer as plain text\n",
        "    system_prompt='Providing a weather forecast at the locations the user provides.',\n",
        ")\n",
        "\n",
        "\n",
        "@weather_agent.tool\n",
        "async def weather_forecast(\n",
        "    ctx: RunContext[WeatherService],\n",
        "    location: str,\n",
        "    forecast_date: date,\n",
        ") -> str:\n",
        "    if forecast_date >= date.today():\n",
        "        return await ctx.deps.get_forecast(location, forecast_date)\n",
        "    else:\n",
        "        return await ctx.deps.get_historic_weather(location, forecast_date)\n",
        "\n",
        "\n",
        "output_messages: list[str] = []\n",
        "\n",
        "\n",
        "async def main():\n",
        "    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n",
        "\n",
        "    # Begin a node-by-node, streaming iteration\n",
        "    async with weather_agent.iter(user_prompt, deps=WeatherService()) as run:\n",
        "        async for node in run:\n",
        "            if Agent.is_user_prompt_node(node):\n",
        "                # A user prompt node => The user has provided input\n",
        "                print(f'=== UserPromptNode: {node.user_prompt} ===')\n",
        "            elif Agent.is_model_request_node(node):\n",
        "                # A model request node => We can stream tokens from the model's request\n",
        "                print(\n",
        "                    '=== ModelRequestNode: streaming partial request tokens ==='\n",
        "                )\n",
        "                async with node.stream(run.ctx) as request_stream:\n",
        "                    async for event in request_stream:\n",
        "                        if isinstance(event, PartStartEvent):\n",
        "                            print(\n",
        "                                f'[Request] Starting part {event.index}: {event.part!r}'\n",
        "                            )\n",
        "                        elif isinstance(event, PartDeltaEvent):\n",
        "                            if isinstance(event.delta, TextPartDelta):\n",
        "                                print(\n",
        "                                    f'[Request] Part {event.index} text delta: {event.delta.content_delta!r}'\n",
        "                                )\n",
        "                            elif isinstance(event.delta, ToolCallPartDelta):\n",
        "                                print(\n",
        "                                    f'[Request] Part {event.index} args_delta={event.delta.args_delta}'\n",
        "                                )\n",
        "                        elif isinstance(event, FinalResultEvent):\n",
        "                            print(\n",
        "                                f'[Result] The model produced a final output (tool_name={event.tool_name})'\n",
        "                            )\n",
        "            elif Agent.is_call_tools_node(node):\n",
        "                # A handle-response node => The model returned some data, potentially calls a tool\n",
        "                print(\n",
        "                    '=== CallToolsNode: streaming partial response & tool usage ==='\n",
        "                )\n",
        "                async with node.stream(run.ctx) as handle_stream:\n",
        "                    async for event in handle_stream:\n",
        "                        if isinstance(event, FunctionToolCallEvent):\n",
        "                            print(\n",
        "                                f'[Tools] The LLM calls tool={event.part.tool_name!r} with args={event.part.args} (tool_call_id={event.part.tool_call_id!r})'\n",
        "                            )\n",
        "                        elif isinstance(event, FunctionToolResultEvent):\n",
        "                            print(\n",
        "                                f'[Tools] Tool call {event.tool_call_id!r} returned => {event.result.content}'\n",
        "                            )\n",
        "            elif Agent.is_end_node(node):\n",
        "                assert run.result.output == node.data.output\n",
        "                # Once an End node is reached, the agent run is complete\n",
        "                print(\n",
        "                    f'=== Final Agent Output: {run.result.output} ==='\n",
        "                )\n",
        "\n",
        "await main()\n"
      ],
      "metadata": {
        "id": "7iHToPFucpvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a94f74-483c-4f46-96f1-fe74d65b0174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== UserPromptNode: What will the weather be like in Paris on Tuesday? ===\n",
            "=== ModelRequestNode: streaming partial request tokens ===\n",
            "[Request] Starting part 0: ToolCallPart(tool_name='weather_forecast', args='', tool_call_id='call_tAG2ZAGtysAIZuHIQmwu7etZ')\n",
            "[Request] Part 0 args_delta={\"\n",
            "[Request] Part 0 args_delta=location\n",
            "[Request] Part 0 args_delta=\":\"\n",
            "[Request] Part 0 args_delta=Paris\n",
            "[Request] Part 0 args_delta=\",\"\n",
            "[Request] Part 0 args_delta=forecast\n",
            "[Request] Part 0 args_delta=_date\n",
            "[Request] Part 0 args_delta=\":\"\n",
            "[Request] Part 0 args_delta=202\n",
            "[Request] Part 0 args_delta=3\n",
            "[Request] Part 0 args_delta=-\n",
            "[Request] Part 0 args_delta=10\n",
            "[Request] Part 0 args_delta=-\n",
            "[Request] Part 0 args_delta=03\n",
            "[Request] Part 0 args_delta=\"}\n",
            "=== CallToolsNode: streaming partial response & tool usage ===\n",
            "[Tools] The LLM calls tool='weather_forecast' with args={\"location\":\"Paris\",\"forecast_date\":\"2023-10-03\"} (tool_call_id='call_tAG2ZAGtysAIZuHIQmwu7etZ')\n",
            "[Tools] Tool call 'call_tAG2ZAGtysAIZuHIQmwu7etZ' returned => The weather in Paris on 2023-10-03 was 18Â°C and partly cloudy.\n",
            "=== ModelRequestNode: streaming partial request tokens ===\n",
            "[Request] Starting part 0: TextPart(content='')\n",
            "[Result] The model produced a final output (tool_name=None)\n",
            "[Request] Part 0 text delta: 'The'\n",
            "[Request] Part 0 text delta: ' weather'\n",
            "[Request] Part 0 text delta: ' in'\n",
            "[Request] Part 0 text delta: ' Paris'\n",
            "[Request] Part 0 text delta: ' on'\n",
            "[Request] Part 0 text delta: ' Tuesday'\n",
            "[Request] Part 0 text delta: ','\n",
            "[Request] Part 0 text delta: ' October'\n",
            "[Request] Part 0 text delta: ' '\n",
            "[Request] Part 0 text delta: '3'\n",
            "[Request] Part 0 text delta: ','\n",
            "[Request] Part 0 text delta: ' '\n",
            "[Request] Part 0 text delta: '202'\n",
            "[Request] Part 0 text delta: '3'\n",
            "[Request] Part 0 text delta: ','\n",
            "[Request] Part 0 text delta: ' will'\n",
            "[Request] Part 0 text delta: ' be'\n",
            "[Request] Part 0 text delta: ' '\n",
            "[Request] Part 0 text delta: '18'\n",
            "[Request] Part 0 text delta: 'Â°C'\n",
            "[Request] Part 0 text delta: ' and'\n",
            "[Request] Part 0 text delta: ' partly'\n",
            "[Request] Part 0 text delta: ' cloudy'\n",
            "[Request] Part 0 text delta: '.'\n",
            "=== CallToolsNode: streaming partial response & tool usage ===\n",
            "=== Final Agent Output: The weather in Paris on Tuesday, October 3, 2023, will be 18Â°C and partly cloudy. ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Airline Service Agent"
      ],
      "metadata": {
        "id": "UgDp9yJT3AvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class Date(BaseModel):\n",
        "    # Somehow LLM is bad at specifying `datetime.datetime`\n",
        "    year: int\n",
        "    month: int\n",
        "    day: int\n",
        "    hour: int\n",
        "\n",
        "class UserProfile(BaseModel):\n",
        "    user_id: str\n",
        "    name: str\n",
        "    email: str\n",
        "\n",
        "class Flight(BaseModel):\n",
        "    flight_id: str\n",
        "    date_time: Date\n",
        "    origin: str\n",
        "    destination: str\n",
        "    duration: float\n",
        "    price: float\n",
        "\n",
        "class Itinerary(BaseModel):\n",
        "    confirmation_number: str\n",
        "    user_profile: UserProfile\n",
        "    flight: Flight\n",
        "\n",
        "class Ticket(BaseModel):\n",
        "    user_request: str\n",
        "    user_profile: UserProfile"
      ],
      "metadata": {
        "id": "JMQDOl6o2D3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_database = {\n",
        "    \"Adam\": UserProfile(user_id=\"1\", name=\"Adam\", email=\"adam@gmail.com\"),\n",
        "    \"Bob\": UserProfile(user_id=\"2\", name=\"Bob\", email=\"bob@gmail.com\"),\n",
        "    \"Chelsie\": UserProfile(user_id=\"3\", name=\"Chelsie\", email=\"chelsie@gmail.com\"),\n",
        "    \"David\": UserProfile(user_id=\"4\", name=\"David\", email=\"david@gmail.com\"),\n",
        "}\n",
        "\n",
        "flight_database = {\n",
        "    \"DA123\": Flight(\n",
        "        flight_id=\"DA123\",\n",
        "        origin=\"SFO\",\n",
        "        destination=\"JFK\",\n",
        "        date_time=Date(year=2025, month=9, day=1, hour=1),\n",
        "        duration=3,\n",
        "        price=200,\n",
        "    ),\n",
        "    \"DA125\": Flight(\n",
        "        flight_id=\"DA125\",\n",
        "        origin=\"SFO\",\n",
        "        destination=\"JFK\",\n",
        "        date_time=Date(year=2025, month=9, day=1, hour=7),\n",
        "        duration=9,\n",
        "        price=500,\n",
        "    ),\n",
        "    \"DA456\": Flight(\n",
        "        flight_id=\"DA456\",\n",
        "        origin=\"SFO\",\n",
        "        destination=\"SNA\",\n",
        "        date_time=Date(year=2025, month=10, day=1, hour=1),\n",
        "        duration=2,\n",
        "        price=100,\n",
        "    ),\n",
        "    \"DA460\": Flight(\n",
        "        flight_id=\"DA460\",\n",
        "        origin=\"SFO\",\n",
        "        destination=\"SNA\",\n",
        "        date_time=Date(year=2025, month=10, day=1, hour=9),\n",
        "        duration=2,\n",
        "        price=120,\n",
        "    ),\n",
        "}\n",
        "\n",
        "itinery_database = {}\n",
        "ticket_database = {}"
      ],
      "metadata": {
        "id": "8b1QnYnV3Ib-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"\"\"\n",
        "    You are an AI customer service agent for Awesome airline, an airline company that runs flights across the globe. Your\n",
        "job is to help users book flights and manage iternerary, including canceling and modifying. When the user request cannot\n",
        "be resolved, make sure you raise a custom support ticket. For the message you return to the user, please include the\n",
        "confirmation number if a flight is booked, an a custom support ticket number if a ticket is raised. On other scenarios,\n",
        "please also make sure that all information users need is included.\n",
        "\n",
        "    Your core principles for interacting with users are:\n",
        "\n",
        "*   **Customer-centricity:** Every interaction should be focused on meeting the customer's needs and resolving their issues.\n",
        "*   **Accuracy:** Ensure all information provided is factually correct and up-to-date, referencing provided tools whenever possible.\n",
        "*   **Efficiency:** Aim to resolve customer issues quickly and effectively, minimizing the need for escalation.\n",
        "*   **Professionalism:** Maintain a courteous and professional tone throughout the conversation.\n",
        "*   **Empathy:** Acknowledge the customer's frustration and show understanding when appropriate.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HNpOQXLK6WmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "airline_agent = Agent(\n",
        "    'openai:gpt-4o-mini',\n",
        "    output_type=str,\n",
        "    system_prompt=instruction,\n",
        ")"
      ],
      "metadata": {
        "id": "8IugdIhA4ylk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "@airline_agent.tool_plain\n",
        "def fetch_flight_info(date: Date, origin: str, destination: str):\n",
        "    \"\"\"Fetch flight information from origin to destination on the given date\"\"\"\n",
        "    flights = []\n",
        "\n",
        "    for flight_id, flight in flight_database.items():\n",
        "        if (\n",
        "            flight.date_time.year == date.year\n",
        "            and flight.date_time.month == date.month\n",
        "            and flight.date_time.day == date.day\n",
        "            and flight.origin == origin\n",
        "            and flight.destination == destination\n",
        "        ):\n",
        "            flights.append(flight)\n",
        "    return flights\n",
        "\n",
        "\n",
        "@airline_agent.tool_plain\n",
        "def fetch_itinerary(confirmation_number: str):\n",
        "    \"\"\"Fetch a booked itinerary information from database\"\"\"\n",
        "    return itinery_database.get(confirmation_number)\n",
        "\n",
        "\n",
        "@airline_agent.tool_plain\n",
        "def pick_flight(flights: list[Flight]):\n",
        "    \"\"\"Pick up the best flight that matches users' request.\"\"\"\n",
        "    sorted_flights = sorted(\n",
        "        flights,\n",
        "        key=lambda x: (\n",
        "            x.get(\"duration\") if isinstance(x, dict) else x.duration,\n",
        "            x.get(\"price\") if isinstance(x, dict) else x.price,\n",
        "        ),\n",
        "    )\n",
        "    return sorted_flights[0]\n",
        "\n",
        "\n",
        "def _generate_id(length=8):\n",
        "    chars = string.ascii_lowercase + string.digits\n",
        "    return \"\".join(random.choices(chars, k=length))\n",
        "\n",
        "\n",
        "@airline_agent.tool_plain\n",
        "def book_itinerary(flight: Flight, user_profile: UserProfile):\n",
        "    \"\"\"Book a flight on behalf of the user.\"\"\"\n",
        "    confirmation_number = _generate_id()\n",
        "    while confirmation_number in itinery_database:\n",
        "        confirmation_number = _generate_id()\n",
        "    itinery_database[confirmation_number] = Itinerary(\n",
        "        confirmation_number=confirmation_number,\n",
        "        user_profile=user_profile,\n",
        "        flight=flight,\n",
        "    )\n",
        "    return confirmation_number, itinery_database[confirmation_number]\n",
        "\n",
        "\n",
        "@airline_agent.tool_plain\n",
        "def cancel_itinerary(confirmation_number: str, user_profile: UserProfile):\n",
        "    \"\"\"Cancel an itinerary on behalf of the user.\"\"\"\n",
        "    if confirmation_number in itinery_database:\n",
        "        del itinery_database[confirmation_number]\n",
        "        return\n",
        "    raise ValueError(\"Cannot find the itinerary, please check your confirmation number.\")\n",
        "\n",
        "\n",
        "@airline_agent.tool_plain\n",
        "def get_user_info(name: str):\n",
        "    \"\"\"Fetch the user profile from database with given name.\"\"\"\n",
        "    return user_database.get(name)\n",
        "\n",
        "\n",
        "@airline_agent.tool_plain\n",
        "def file_ticket(user_request: str, user_profile: UserProfile):\n",
        "    \"\"\"File a customer support ticket if this is something the agent cannot handle.\"\"\"\n",
        "    ticket_id = _generate_id(length=6)\n",
        "    ticket_database[ticket_id] = Ticket(\n",
        "        user_request=user_request,\n",
        "        user_profile=user_profile,\n",
        "    )\n",
        "    return ticket_id\n"
      ],
      "metadata": {
        "id": "8GBtI0nP3I9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = await airline_agent.run(\"please help me book a flight from SFO to JFK on 09/01/2025, my name is Adam\")"
      ],
      "metadata": {
        "id": "i7YPUM2P6tMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucF9zLaw7ICz",
        "outputId": "75693621-6fbe-4311-b798-90276deee8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentRunResult(output='Your flight from San Francisco (SFO) to New York City (JFK) has been successfully booked! Here are the details:\\n\\n- **Flight ID:** DA123\\n- **Departure Date and Time:** September 1, 2025, at 1:00 AM\\n- **Duration:** 3 hours\\n- **Price:** $200\\n\\nYour **confirmation number** is **x7z20qsz**.\\n\\nIf you have any other requests or need further assistance, feel free to ask!')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "itinery_database = {}"
      ],
      "metadata": {
        "id": "98frj9VK8nso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async with airline_agent.iter(\"please help me book a flight from SFO to JFK on 09/01/2025, my name is Adam\") as agent_run:\n",
        "    async for node in agent_run:\n",
        "        print(f\"GEEZ NODE: {node}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipC6y1Kh7O5y",
        "outputId": "c41834b6-173f-4f29-c0e9-42c28b6376de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "00:31:08.515 airline_agent run\n",
            "GEEZ NODE: UserPromptNode(user_prompt='please help me book a flight from SFO to JFK on 09/01/2025, my name is Adam', instructions=None, instructions_functions=[], system_prompts=(\"\\n    You are an AI customer service agent for Awesome airline, an airline company that runs flights across the globe. Your\\njob is to help users book flights and manage iternerary, including canceling and modifying. When the user request cannot\\nbe resolved, make sure you raise a custom support ticket. For the message you return to the user, please include the\\nconfirmation number if a flight is booked, an a custom support ticket number if a ticket is raised. On other scenarios,\\nplease also make sure that all information users need is included.\\n\\n    Your core principles for interacting with users are:\\n\\n*   **Customer-centricity:** Every interaction should be focused on meeting the customer's needs and resolving their issues.\\n*   **Accuracy:** Ensure all information provided is factually correct and up-to-date, referencing provided tools whenever possible.\\n*   **Efficiency:** Aim to resolve customer issues quickly and effectively, minimizing the need for escalation.\\n*   **Professionalism:** Maintain a courteous and professional tone throughout the conversation.\\n*   **Empathy:** Acknowledge the customer's frustration and show understanding when appropriate.\\n\",), system_prompt_functions=[], system_prompt_dynamic_functions={})\n",
            "GEEZ NODE: ModelRequestNode(request=ModelRequest(parts=[SystemPromptPart(content=\"\\n    You are an AI customer service agent for Awesome airline, an airline company that runs flights across the globe. Your\\njob is to help users book flights and manage iternerary, including canceling and modifying. When the user request cannot\\nbe resolved, make sure you raise a custom support ticket. For the message you return to the user, please include the\\nconfirmation number if a flight is booked, an a custom support ticket number if a ticket is raised. On other scenarios,\\nplease also make sure that all information users need is included.\\n\\n    Your core principles for interacting with users are:\\n\\n*   **Customer-centricity:** Every interaction should be focused on meeting the customer's needs and resolving their issues.\\n*   **Accuracy:** Ensure all information provided is factually correct and up-to-date, referencing provided tools whenever possible.\\n*   **Efficiency:** Aim to resolve customer issues quickly and effectively, minimizing the need for escalation.\\n*   **Professionalism:** Maintain a courteous and professional tone throughout the conversation.\\n*   **Empathy:** Acknowledge the customer's frustration and show understanding when appropriate.\\n\", timestamp=datetime.datetime(2025, 6, 15, 0, 31, 8, 516457, tzinfo=datetime.timezone.utc)), UserPromptPart(content='please help me book a flight from SFO to JFK on 09/01/2025, my name is Adam', timestamp=datetime.datetime(2025, 6, 15, 0, 31, 8, 516463, tzinfo=datetime.timezone.utc))]))\n",
            "00:31:08.517   chat gpt-4o-mini\n",
            "GEEZ NODE: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='get_user_info', args='{\"name\":\"Adam\"}', tool_call_id='call_BGqJJV9cBfT8Y8pOaVZbpAm8')], usage=Usage(requests=1, request_tokens=608, response_tokens=15, total_tokens=623, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0, 'cached_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 6, 15, 0, 31, 8, tzinfo=TzInfo(UTC)), vendor_id='chatcmpl-BiVZ6eCnkgTe7B5f8nCscqst9dHTF'))\n",
            "00:31:09.354   running 1 tool\n",
            "00:31:09.354     running tool: get_user_info\n",
            "GEEZ NODE: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='get_user_info', content=UserProfile(user_id='1', name='Adam', email='adam@gmail.com'), tool_call_id='call_BGqJJV9cBfT8Y8pOaVZbpAm8', timestamp=datetime.datetime(2025, 6, 15, 0, 31, 9, 356610, tzinfo=datetime.timezone.utc))]))\n",
            "00:31:09.358   chat gpt-4o-mini\n",
            "GEEZ NODE: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='fetch_flight_info', args='{\"date\":{\"year\":2025,\"month\":9,\"day\":1,\"hour\":0},\"origin\":\"SFO\",\"destination\":\"JFK\"}', tool_call_id='call_KkXAHB9pAMTF3RwjUJsHG2p4')], usage=Usage(requests=1, request_tokens=648, response_tokens=41, total_tokens=689, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0, 'cached_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 6, 15, 0, 31, 9, tzinfo=TzInfo(UTC)), vendor_id='chatcmpl-BiVZ7CmEGxKvqvFak4pKHcaQ7nQYg'))\n",
            "00:31:10.414   running 1 tool\n",
            "00:31:10.414     running tool: fetch_flight_info\n",
            "GEEZ NODE: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='fetch_flight_info', content=[Flight(flight_id='DA123', date_time=Date(year=2025, month=9, day=1, hour=1), origin='SFO', destination='JFK', duration=3.0, price=200.0), Flight(flight_id='DA125', date_time=Date(year=2025, month=9, day=1, hour=7), origin='SFO', destination='JFK', duration=9.0, price=500.0)], tool_call_id='call_KkXAHB9pAMTF3RwjUJsHG2p4', timestamp=datetime.datetime(2025, 6, 15, 0, 31, 10, 415655, tzinfo=datetime.timezone.utc))]))\n",
            "00:31:10.417   chat gpt-4o-mini\n",
            "GEEZ NODE: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='pick_flight', args='{\"flights\":[{\"flight_id\":\"DA123\",\"date_time\":{\"year\":2025,\"month\":9,\"day\":1,\"hour\":1},\"origin\":\"SFO\",\"destination\":\"JFK\",\"duration\":3.0,\"price\":200.0},{\"flight_id\":\"DA125\",\"date_time\":{\"year\":2025,\"month\":9,\"day\":1,\"hour\":7},\"origin\":\"SFO\",\"destination\":\"JFK\",\"duration\":9.0,\"price\":500.0}]}', tool_call_id='call_ez40LJ1N2aYRAxoyNQt0wNWV')], usage=Usage(requests=1, request_tokens=797, response_tokens=112, total_tokens=909, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0, 'cached_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 6, 15, 0, 31, 10, tzinfo=TzInfo(UTC)), vendor_id='chatcmpl-BiVZ8TKIqHXRGvxq7q4m7IenmnCMe'))\n",
            "00:31:13.091   running 1 tool\n",
            "00:31:13.091     running tool: pick_flight\n",
            "GEEZ NODE: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='pick_flight', content=Flight(flight_id='DA123', date_time=Date(year=2025, month=9, day=1, hour=1), origin='SFO', destination='JFK', duration=3.0, price=200.0), tool_call_id='call_ez40LJ1N2aYRAxoyNQt0wNWV', timestamp=datetime.datetime(2025, 6, 15, 0, 31, 13, 93549, tzinfo=datetime.timezone.utc))]))\n",
            "00:31:13.095   chat gpt-4o-mini\n",
            "GEEZ NODE: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='book_itinerary', args='{\"flight\":{\"flight_id\":\"DA123\",\"date_time\":{\"year\":2025,\"month\":9,\"day\":1,\"hour\":1},\"origin\":\"SFO\",\"destination\":\"JFK\",\"duration\":3.0,\"price\":200.0},\"user_profile\":{\"user_id\":\"1\",\"name\":\"Adam\",\"email\":\"adam@gmail.com\"}}', tool_call_id='call_JSHSJMIi3lGQWe63lKbsm1Wt')], usage=Usage(requests=1, request_tokens=967, response_tokens=79, total_tokens=1046, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0, 'cached_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 6, 15, 0, 31, 13, tzinfo=TzInfo(UTC)), vendor_id='chatcmpl-BiVZBG8OacxBaeImLMLvxRU6vntMQ'))\n",
            "00:31:14.742   running 1 tool\n",
            "00:31:14.743     running tool: book_itinerary\n",
            "GEEZ NODE: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='book_itinerary', content=('dij6f6g5', Itinerary(confirmation_number='dij6f6g5', user_profile=UserProfile(user_id='1', name='Adam', email='adam@gmail.com'), flight=Flight(flight_id='DA123', date_time=Date(year=2025, month=9, day=1, hour=1), origin='SFO', destination='JFK', duration=3.0, price=200.0))), tool_call_id='call_JSHSJMIi3lGQWe63lKbsm1Wt', timestamp=datetime.datetime(2025, 6, 15, 0, 31, 14, 744741, tzinfo=datetime.timezone.utc))]))\n",
            "00:31:14.746   chat gpt-4o-mini\n",
            "GEEZ NODE: CallToolsNode(model_response=ModelResponse(parts=[TextPart(content='Your flight from San Francisco (SFO) to New York City (JFK) has been successfully booked! Here are the details:\\n\\n- **Flight ID:** DA123\\n- **Departure:** September 1, 2025, at 1:00 AM\\n- **Duration:** 3 hours\\n- **Price:** $200\\n- **Confirmation Number:** dij6f6g5\\n\\nIf you have any further questions or need assistance, feel free to ask!')], usage=Usage(requests=1, request_tokens=1146, response_tokens=97, total_tokens=1243, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0, 'cached_tokens': 1024}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 6, 15, 0, 31, 14, tzinfo=TzInfo(UTC)), vendor_id='chatcmpl-BiVZCVIwtNPcp0h65OfcTFIQu0FZT'))\n",
            "GEEZ NODE: End(data=FinalResult(output='Your flight from San Francisco (SFO) to New York City (JFK) has been successfully booked! Here are the details:\\n\\n- **Flight ID:** DA123\\n- **Departure:** September 1, 2025, at 1:00 AM\\n- **Duration:** 3 hours\\n- **Price:** $200\\n- **Confirmation Number:** dij6f6g5\\n\\nIf you have any further questions or need assistance, feel free to ask!'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically pydantic AI puts tool call and LM call into the term called \"Node\", and by using the async generator, we are iterating through the node processing. The cool thing is it gives flexiblity, while the uncool thing is it's not super clear how I should play with these nodes at the first place.\n",
        "\n",
        "Streaming is very powerful, with streaming support on structured output fields, while I am not sure why I need that. Still impressive though.\n",
        "\n",
        "The concept is a bit overwhelming to be honest, like the deps, context and different ways of configuring tools, and all those attributes available in a node. All these are like specific terms to Pydantic AI, so they are sort of \"made up\". With that, it's pretty much means I need to learn a lot about pydantic AI in order to get things up and running. This is similar to DSPy where we keep things flexible for the user, but PydanticAI has way more things to learn than DSPy.\n",
        "\n",
        "The logging experience is pretty much DIY. Tracing experience is decent, which is powered by something called logfire."
      ],
      "metadata": {
        "id": "zp914VKdAHgS"
      }
    }
  ]
}